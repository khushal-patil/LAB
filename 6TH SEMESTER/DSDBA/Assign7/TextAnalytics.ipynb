{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt8jHJSZEr88"
   },
   "source": [
    "**1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vrPSIa8zlNE"
   },
   "source": [
    "Step1: Downloading and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3302,
     "status": "ok",
     "timestamp": 1737696278856,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "vcTq2Lu2sO3u",
    "outputId": "b73dbfb5-b160-4141-d767-57c248062e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2642,
     "status": "ok",
     "timestamp": 1739514624583,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": 480
    },
    "id": "yv0L20Isx1z9",
    "outputId": "6be7b7b4-6517-4734-c487-a8c403fd5140"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC_vccD3zTGo"
   },
   "source": [
    "**Step 2: Creating a Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1737697560940,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "qxk2dR1GzwNM",
    "outputId": "3aa3b0d2-7a77-47e5-b04c-a768213917fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, Everyone myself khushal patil from dhule.', \"I am currently pursuing my bachelor's from MES Wadia College of Engineering, Pune in Computer Engineering.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi, Everyone myself khushal patil from dhule. I am currently pursuing my bachelor's from MES Wadia College of Engineering, Pune in Computer Engineering.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = sent_tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1737697579219,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "Kuuw7J4m2GM6",
    "outputId": "582588cd-830a-4027-bb19-eed1b4332821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'Everyone', 'myself', 'khushal', 'patil', 'from', 'dhule', '.', 'I', 'am', 'currently', 'pursuing', 'my', 'bachelor', \"'s\", 'from', 'MES', 'Wadia', 'College', 'of', 'Engineering', ',', 'Pune', 'in', 'Computer', 'Engineering', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize_text = word_tokenize(text)\n",
    "print(word_tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1737697807845,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "2GaXHgCk2vcr",
    "outputId": "92b98d51-6374-4ff3-e7e8-d038b8a8d0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'we', 'other', 'shan', 'down', 'so', 'not', 'needn', 'this', 'wasn', 'my', 'himself', 'if', 'through', \"don't\", 't', 'y', 'hadn', \"it's\", 've', 'with', \"you're\", 'off', \"doesn't\", \"won't\", 'about', 'the', 'what', 'after', 'his', 'here', 'do', 'themselves', \"hadn't\", 'nor', 'for', \"couldn't\", 'than', 'only', 'won', 'mightn', 'am', 'm', 'myself', 'does', 'her', 'between', 'didn', \"she's\", 'theirs', 'same', 'into', 'whom', 'but', 'there', 'because', 'too', 'mustn', 'now', 'just', 'had', 'hasn', \"you'll\", \"shan't\", 'as', 's', 'during', 'at', 'wouldn', 'under', \"wouldn't\", 'some', 'she', \"didn't\", 'our', 'hers', 'of', 'ourselves', 'having', 'was', 'which', 'very', 'below', 'its', 'are', \"mightn't\", 'your', 'further', \"wasn't\", 'they', \"needn't\", 'he', 'where', 'most', 'them', 'ain', \"you've\", 'more', \"hasn't\", 'shouldn', 'be', 'why', 'aren', 'those', 'before', 'each', 'both', 'should', 'll', 'a', 'him', 'been', 'yours', 'herself', 'it', 'then', 'd', 'doesn', \"that'll\", 'haven', 'will', 'few', 'weren', 'itself', 'once', \"mustn't\", 'being', \"should've\", 'against', 'is', \"isn't\", 'over', 'and', 're', \"aren't\", \"shouldn't\", \"weren't\", 'were', 'an', 'until', 'on', 'their', 'have', 'such', 'you', 'again', 'ours', 'any', 'no', \"you'd\", 'or', 'yourselves', 'all', 'ma', \"haven't\", 'own', 'while', 'when', 'can', 'o', 'doing', 'by', 'from', 'that', 'up', 'i', 'these', 'isn', 'did', 'has', 'how', 'don', 'couldn', 'yourself', 'me', 'out', 'who', 'in', 'above'}\n"
     ]
    }
   ],
   "source": [
    "#print stop words of English\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1737698289090,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "ifUSxTLA3Pfq",
    "outputId": "0206bc30-2ff8-4590-8561-e76c4c30128b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence:  ['hi', 'everyone', 'myself', 'khushal', 'patil', 'from', 'dhule', 'i', 'am', 'currently', 'pursuing', 'my', 'bachelor', 's', 'from', 'mes', 'wadia', 'college', 'of', 'engineering', 'pune', 'in', 'computer', 'engineering']\n",
      "Filtered Sentence:  ['hi', 'everyone', 'khushal', 'patil', 'dhule', 'currently', 'pursuing', 'bachelor', 'mes', 'wadia', 'college', 'engineering', 'pune', 'computer', 'engineering']\n"
     ]
    }
   ],
   "source": [
    "text = re.sub('[^a-zA-Z]',' ',text)\n",
    "\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text = []\n",
    "for w in tokens:\n",
    "  if w not in stop_words:\n",
    "    filtered_text.append(w)\n",
    "print(\"Tokenized Sentence: \", tokens)\n",
    "print(\"Filtered Sentence: \", filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f2WxJaL66NX"
   },
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1737699565310,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "UGG09BBo6mbo",
    "outputId": "4c467c42-1480-4f1e-9820-06cff9a50e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words = [\"write\",\"writing\",\"wrote\",\"writes\"]\n",
    "ps = PorterStemmer()\n",
    "for w in e_words:\n",
    "  rootWord=ps.stem(w)\n",
    "print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_e_zv--9HmY"
   },
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1737699868296,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": -330
    },
    "id": "OnXHLdsg9K5n",
    "outputId": "b7f42cff-94cf-4d91-adc2-2ef54c0d5dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies studying cries cry\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tt = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "\n",
    "for w in tt:\n",
    "  print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POoayihH0r_e"
   },
   "source": [
    "**2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1739514903406,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": 480
    },
    "id": "Io41htbN1Pqd",
    "outputId": "fcecc1df-bb82-4490-dc9f-260571e3d4cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use', 'the', 'in', 'inbuilt', 'dataset', 'titanic', 'The', 'dataset', 'contains', '891', 'rows', 'and', 'contains', 'information', 'about', 'the', 'passengers', 'who', 'boarded', 'the', 'unfortunate', 'Titanic', 'ship', 'Use', 'the', 'Seaborn', 'library', 'to', 'see', 'if', 'we', 'can', 'find', 'any', 'patterns', 'in', 'the', 'data']\n",
      "['Hi', 'Everyone', 'myself', 'khushal', 'patil', 'from', 'dhule', 'I', 'am', 'currently', 'pursuing', 'my', 'bachelors', 'from', 'MES', 'Wadia', 'College', 'of', 'Engineering', 'Pune', 'in', 'Computer', 'Engineering']\n",
      "Unique Words:  {'in', 'ship', 'if', 'unfortunate', 'data', 'Everyone', 'contains', 'the', 'College', 'information', 'library', 'Seaborn', 'can', 'my', 'patterns', 'rows', 'who', 'patil', '891', 'khushal', 'any', 'about', 'dataset', 'find', 'Pune', 'we', 'to', 'Engineering', 'and', 'titanic', 'Titanic', 'Hi', 'from', 'bachelors', 'inbuilt', 'I', 'of', 'dhule', 'Computer', 'passengers', 'pursuing', 'Wadia', 'am', 'currently', 'myself', 'see', 'Use', 'boarded', 'MES', 'The'} \n",
      "\n",
      "\n",
      "{'in': 2, 'ship': 1, 'if': 1, 'unfortunate': 1, 'data': 1, 'Everyone': 0, 'contains': 2, 'the': 5, 'College': 0, 'information': 1, 'library': 1, 'Seaborn': 1, 'can': 1, 'my': 0, 'patterns': 1, 'rows': 1, 'who': 1, 'patil': 0, '891': 1, 'khushal': 0, 'any': 1, 'about': 1, 'dataset': 2, 'find': 1, 'Pune': 0, 'we': 1, 'to': 1, 'Engineering': 0, 'and': 1, 'titanic': 1, 'Titanic': 1, 'Hi': 0, 'from': 0, 'bachelors': 0, 'inbuilt': 1, 'I': 0, 'of': 0, 'dhule': 0, 'Computer': 0, 'passengers': 1, 'pursuing': 0, 'Wadia': 0, 'am': 0, 'currently': 0, 'myself': 0, 'see': 1, 'Use': 2, 'boarded': 1, 'MES': 0, 'The': 1}\n",
      "{'in': 1, 'ship': 0, 'if': 0, 'unfortunate': 0, 'data': 0, 'Everyone': 1, 'contains': 0, 'the': 0, 'College': 1, 'information': 0, 'library': 0, 'Seaborn': 0, 'can': 0, 'my': 1, 'patterns': 0, 'rows': 0, 'who': 0, 'patil': 1, '891': 0, 'khushal': 1, 'any': 0, 'about': 0, 'dataset': 0, 'find': 0, 'Pune': 1, 'we': 0, 'to': 0, 'Engineering': 2, 'and': 0, 'titanic': 0, 'Titanic': 0, 'Hi': 1, 'from': 2, 'bachelors': 1, 'inbuilt': 0, 'I': 1, 'of': 1, 'dhule': 1, 'Computer': 1, 'passengers': 0, 'pursuing': 1, 'Wadia': 1, 'am': 1, 'currently': 1, 'myself': 1, 'see': 0, 'Use': 0, 'boarded': 0, 'MES': 1, 'The': 0}\n",
      "TF of A:  {'in': 0.05263157894736842, 'ship': 0.02631578947368421, 'if': 0.02631578947368421, 'unfortunate': 0.02631578947368421, 'data': 0.02631578947368421, 'Everyone': 0.0, 'contains': 0.05263157894736842, 'the': 0.13157894736842105, 'College': 0.0, 'information': 0.02631578947368421, 'library': 0.02631578947368421, 'Seaborn': 0.02631578947368421, 'can': 0.02631578947368421, 'my': 0.0, 'patterns': 0.02631578947368421, 'rows': 0.02631578947368421, 'who': 0.02631578947368421, 'patil': 0.0, '891': 0.02631578947368421, 'khushal': 0.0, 'any': 0.02631578947368421, 'about': 0.02631578947368421, 'dataset': 0.05263157894736842, 'find': 0.02631578947368421, 'Pune': 0.0, 'we': 0.02631578947368421, 'to': 0.02631578947368421, 'Engineering': 0.0, 'and': 0.02631578947368421, 'titanic': 0.02631578947368421, 'Titanic': 0.02631578947368421, 'Hi': 0.0, 'from': 0.0, 'bachelors': 0.0, 'inbuilt': 0.02631578947368421, 'I': 0.0, 'of': 0.0, 'dhule': 0.0, 'Computer': 0.0, 'passengers': 0.02631578947368421, 'pursuing': 0.0, 'Wadia': 0.0, 'am': 0.0, 'currently': 0.0, 'myself': 0.0, 'see': 0.02631578947368421, 'Use': 0.05263157894736842, 'boarded': 0.02631578947368421, 'MES': 0.0, 'The': 0.02631578947368421}\n",
      "TF of B:  {'in': 0.043478260869565216, 'ship': 0.0, 'if': 0.0, 'unfortunate': 0.0, 'data': 0.0, 'Everyone': 0.043478260869565216, 'contains': 0.0, 'the': 0.0, 'College': 0.043478260869565216, 'information': 0.0, 'library': 0.0, 'Seaborn': 0.0, 'can': 0.0, 'my': 0.043478260869565216, 'patterns': 0.0, 'rows': 0.0, 'who': 0.0, 'patil': 0.043478260869565216, '891': 0.0, 'khushal': 0.043478260869565216, 'any': 0.0, 'about': 0.0, 'dataset': 0.0, 'find': 0.0, 'Pune': 0.043478260869565216, 'we': 0.0, 'to': 0.0, 'Engineering': 0.08695652173913043, 'and': 0.0, 'titanic': 0.0, 'Titanic': 0.0, 'Hi': 0.043478260869565216, 'from': 0.08695652173913043, 'bachelors': 0.043478260869565216, 'inbuilt': 0.0, 'I': 0.043478260869565216, 'of': 0.043478260869565216, 'dhule': 0.043478260869565216, 'Computer': 0.043478260869565216, 'passengers': 0.0, 'pursuing': 0.043478260869565216, 'Wadia': 0.043478260869565216, 'am': 0.043478260869565216, 'currently': 0.043478260869565216, 'myself': 0.043478260869565216, 'see': 0.0, 'Use': 0.0, 'boarded': 0.0, 'MES': 0.043478260869565216, 'The': 0.0}\n"
     ]
    }
   ],
   "source": [
    "#program for calculating TF\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documentA = 'Use the in inbuilt dataset titanic The dataset contains 891 rows and contains information about the passengers who boarded the unfortunate Titanic ship Use the Seaborn library to see if we can find any patterns in the data'\n",
    "documentB = 'Hi Everyone myself khushal patil from dhule I am currently pursuing my bachelors from MES Wadia College of Engineering Pune in Computer Engineering'\n",
    "\n",
    "\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "print(bagOfWordsA)\n",
    "print(bagOfWordsB)\n",
    "\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "print( \"Unique Words: \",uniqueWords,\"\\n\\n\")\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords,0)\n",
    "#print(numOfWordsA)\n",
    "\n",
    "for word in bagOfWordsA:\n",
    "  numOfWordsA[word] += 1\n",
    "print(numOfWordsA)\n",
    "\n",
    "numOfWordsB = dict.fromkeys(uniqueWords,0)\n",
    "for word in bagOfWordsB:\n",
    "  numOfWordsB[word] +=1\n",
    "print(numOfWordsB)\n",
    "\n",
    "def computeTF(wordDict,bagOfWords):\n",
    "  tfDict = {}\n",
    "  bagOfWordsCount = len(bagOfWords)\n",
    "\n",
    "  for word,count in wordDict.items():\n",
    "    tfDict[word] = count/float(bagOfWordsCount)\n",
    "  return tfDict\n",
    "\n",
    "\n",
    "tfA = computeTF(numOfWordsA,bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB,bagOfWordsB)\n",
    "\n",
    "print(\"TF of A: \",tfA)\n",
    "print(\"TF of B: \",tfB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739514834359,
     "user": {
      "displayName": "khushal patil",
      "userId": "08148323287390017325"
     },
     "user_tz": 480
    },
    "id": "OPsqR01h1PhV",
    "outputId": "db8603b2-021e-425e-f6d5-8fa3f4d3dae6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 0.0,\n",
       " 'ship': 1,\n",
       " 'if': 1,\n",
       " 'unfortunate': 1,\n",
       " 'data': 1,\n",
       " 'Everyone': 1,\n",
       " 'contains': 1,\n",
       " 'the': 1,\n",
       " 'College': 1,\n",
       " 'information': 1,\n",
       " 'library': 1,\n",
       " 'Seaborn': 1,\n",
       " 'can': 1,\n",
       " 'my': 1,\n",
       " 'patterns': 1,\n",
       " 'rows': 1,\n",
       " 'who': 1,\n",
       " 'patil': 1,\n",
       " '891': 1,\n",
       " 'khushal': 1,\n",
       " 'any': 1,\n",
       " 'about': 1,\n",
       " 'dataset': 1,\n",
       " 'find': 1,\n",
       " 'Pune': 1,\n",
       " 'we': 1,\n",
       " 'to': 1,\n",
       " 'Engineering': 1,\n",
       " 'and': 1,\n",
       " 'titanic': 1,\n",
       " 'Titanic': 1,\n",
       " 'Hi': 1,\n",
       " 'from': 1,\n",
       " 'bachelors': 1,\n",
       " 'inbuilt': 1,\n",
       " 'I': 1,\n",
       " 'of': 1,\n",
       " 'dhule': 1,\n",
       " 'Computer': 1,\n",
       " 'passengers': 1,\n",
       " 'pursuing': 1,\n",
       " 'Wadia': 1,\n",
       " 'am': 1,\n",
       " 'currently': 1,\n",
       " 'myself': 1,\n",
       " 'see': 1,\n",
       " 'Use': 1,\n",
       " 'boarded': 1,\n",
       " 'MES': 1,\n",
       " 'The': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating inverse document frequency\n",
    "def computeIDF(documents):\n",
    "  import math\n",
    "  N = len(documents)\n",
    "\n",
    "  idfDict = dict.fromkeys(documents[0].keys(),0)\n",
    "  for document in documents:\n",
    "    for word,val in document.items():\n",
    "      if val > 0:\n",
    "        idfDict[word] +=1\n",
    "\n",
    "  for word,val in idfDict.items():\n",
    "    idfDict[word] = math.log(N/float(val))\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([numOfWordsA,numOfWordsB])\n",
    "idfs\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1w/Zn/664QNv8R5LPGKfS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
