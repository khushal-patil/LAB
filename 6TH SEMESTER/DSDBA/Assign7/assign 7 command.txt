!pip install nltk

import nltk
import re
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')

text = "Hi, Everyone myself khushal patil from dhule. I am currently pursuing my bachelor's from MES Wadia College of Engineering, Pune in Computer Engineering."

# Sentence Tokenization
from nltk.tokenize import sent_tokenize
tokenized_text = sent_tokenize(text)
print(tokenized_text)

#Word Tokenization
from nltk.tokenize import word_tokenize
word_tokenize_text = word_tokenize(text)
print(word_tokenize_text)

#print stop words of English
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)

text = re.sub('[^a-zA-Z]',' ',text)


tokens = word_tokenize(text.lower())
filtered_text = []
for w in tokens:
  if w not in stop_words:
    filtered_text.append(w)
print("Tokenized Sentence: ", tokens)
print("Filtered Sentence: ", filtered_text)

from nltk.stem import PorterStemmer
e_words = ["write","writing","wrote","writes"]
ps = PorterStemmer()
for w in e_words:
  rootWord=ps.stem(w)
print(rootWord)

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tt = nltk.word_tokenize(text)
print(text)

for w in tt:
  print("Lemma for {} is {}".format(w,wordnet_lemmatizer.lemmatize(w)))
  
#program for calculating TF
import pandas as pd
import math
from sklearn.feature_extraction.text import TfidfVectorizer

documentA = 'Use the in inbuilt dataset titanic The dataset contains 891 rows and contains information about the passengers who boarded the unfortunate Titanic ship Use the Seaborn library to see if we can find any patterns in the data'
documentB = 'Hi Everyone myself khushal patil from dhule I am currently pursuing my bachelors from MES Wadia College of Engineering Pune in Computer Engineering'


bagOfWordsA = documentA.split(' ')
bagOfWordsB = documentB.split(' ')
print(bagOfWordsA)
print(bagOfWordsB)

uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))
print( "Unique Words: ",uniqueWords,"\n\n")

numOfWordsA = dict.fromkeys(uniqueWords,0)
#print(numOfWordsA)

for word in bagOfWordsA:
  numOfWordsA[word] += 1
print(numOfWordsA)

numOfWordsB = dict.fromkeys(uniqueWords,0)
for word in bagOfWordsB:
  numOfWordsB[word] +=1
print(numOfWordsB)

def computeTF(wordDict,bagOfWords):
  tfDict = {}
  bagOfWordsCount = len(bagOfWords)

  for word,count in wordDict.items():
    tfDict[word] = count/float(bagOfWordsCount)
  return tfDict


tfA = computeTF(numOfWordsA,bagOfWordsA)
tfB = computeTF(numOfWordsB,bagOfWordsB)

print("TF of A: ",tfA)
print("TF of B: ",tfB)


# calculating inverse document frequency
def computeIDF(documents):
  import math
  N = len(documents)

  idfDict = dict.fromkeys(documents[0].keys(),0)
  for document in documents:
    for word,val in document.items():
      if val > 0:
        idfDict[word] +=1

  for word,val in idfDict.items():
    idfDict[word] = math.log(N/float(val))
    return idfDict

idfs = computeIDF([numOfWordsA,numOfWordsB])
idfs
 
  
