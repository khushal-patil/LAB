# Start Hadoop DFS services (if not already running)
sudo su hadoop
start-dfs.sh

# Check if the services are running
jps

sudo chown -R $(whoami):$(whoami) classes

# Compile the Java program
javac -classpath "$(hadoop classpath)" -d classes WordCount.java

# Create the JAR file
jar cf wordcount.jar -C classes/ .

# Remove any existing input/output in HDFS
hdfs dfs -rm -r input/sample.txt
hdfs dfs -rm -r output

# Create input directory in HDFS and put the sample text file
hdfs dfs -mkdir -p input
hdfs dfs -put /home/khushal/WordCount/input/sample.txt input/

# Run the WordCount program with Hadoop
hadoop jar wordcount.jar WordCount input/sample.txt output

# Retrieve the output from HDFS to the local machine
hdfs dfs -get output/part-r-00000 ./output/result.txt



export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
sudo update-alternatives --config java
 545  sudo chmod -R o+r /home/khushal/WordCount/input/sample.txt
  546  sudo chmod o+x /home/khushal /home/khushal/WordCount /home/khushal/WordCount/input
  547  sudo chmod -R o+r /home/khushal/WordCount/input/sample.txt
  548  sudo chmod o+x /home/khushal /home/khushal/WordCount /home/khushal/WordCount/input
 538  sudo chown -R hadoop:hadoop /home/khushal/WordCount
 
 
 
 hadoop installation steps
java -version

sudo apt install ssh

sudo apt-get install pdsh

sudo adduser hadoop

su — hadoop

ssh-keygen -t rsa

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 

chmod 640 ~/.ssh/authorized_keys

ssh localhost

su — hadoop

wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz

tar -xvzf hadoop-3.4.0.tar.gz

mv hadoop-3.4.0 hadoop


nano ~/.bashrc

	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
	export HADOOP_HOME=/home/hadoop/hadoop
	export HADOOP_INSTALL=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export HADOOP_YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

source ~/.bashrc	

nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

	JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

cd hadoop/

nano $HADOOP_HOME/etc/hadoop/core-site.xml

	<configuration>
	    <property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
	    </property>
	</configuration>

nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
	<configuration>
	    <property>
		<name>dfs.replication</name>
		<value>1</value>
	    </property>
	    <property>
		<name>dfs.namenode.name.dir</name>
		<value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>
	    </property>
	    <property>
		<name>dfs.datanode.data.dir</name>
		<value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>
	    </property>
	 </configuration>
	 
bin/hdfs namenode -format
	
sbin/start-dfs.sh
